"""
Baseline í•™ìŠµ ê²°ê³¼ ê²€ì¦ ë„êµ¬

í•™ìŠµ ì§í›„ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì§„ë‹¨í•˜ê³  ë‹¤ìŒì„ ê²€ì¦:
1. ì”ì°¨ ì§„ë‹¨ (Ljung-Box, ACF, ì •ê·œì„±)
2. ê¸°ì¤€ ì§€í‘œ (MAPE, MASE, Bias)
3. í´ë°± ëª¨ë¸ ì‚¬ìš©ë¥ 

ì‹¤í–‰ ì˜ˆì‹œ:
    python tools/validate_baseline.py --year 2024
    python tools/validate_baseline.py --artifacts artifacts/
"""

import sys
import argparse
from pathlib import Path
import pandas as pd
import numpy as np
import warnings
from datetime import datetime
from typing import Dict, List, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.metrics import (
    compute_all_metrics, 
    compute_metrics_by_group,
    identify_poor_performers
)

warnings.filterwarnings('ignore')


def load_forecast_results(artifacts_path: Path, year: int) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    ì˜ˆì¸¡ ê²°ê³¼ ë° ì‹¤ì¸¡ê°’ ë¡œë“œ
    
    Args:
        artifacts_path: ì•„í‹°íŒ©íŠ¸ ë””ë ‰í† ë¦¬
        year: ê²€ì¦ ì—°ë„
    
    Returns:
        (forecast_df, actual_df) íŠœí”Œ
    """
    forecast_path = artifacts_path / 'forecasts' / f'forecast_{year}.parquet'
    
    if not forecast_path.exists():
        raise FileNotFoundError(f"ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {forecast_path}")
    
    forecast_df = pd.read_parquet(forecast_path)
    
    # ì‹¤ì¸¡ê°’ ë¡œë“œ (curated ë°ì´í„°ì—ì„œ)
    curated_path = project_root / 'data' / 'curated' / f'curated_{year}.parquet'
    if curated_path.exists():
        actual_df = pd.read_parquet(curated_path)
    else:
        print(f"âš ï¸  ì‹¤ì¸¡ê°’ì´ ì—†ìŠµë‹ˆë‹¤: {curated_path}")
        actual_df = None
    
    return forecast_df, actual_df


def analyze_residuals(forecast_df: pd.DataFrame, actual_df: pd.DataFrame) -> pd.DataFrame:
    """
    ì”ì°¨ ë¶„ì„: Ljung-Box í…ŒìŠ¤íŠ¸, ACF, ì •ê·œì„±
    
    Args:
        forecast_df: ì˜ˆì¸¡ ë°ì´í„°í”„ë ˆì„
        actual_df: ì‹¤ì¸¡ ë°ì´í„°í”„ë ˆì„
    
    Returns:
        ì”ì°¨ ë¶„ì„ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
    """
    from scipy import stats
    from statsmodels.stats.diagnostic import acorr_ljungbox
    
    results = []
    
    # ì‹œë¦¬ì¦ˆë³„ ë¶„ì„
    for series_id in forecast_df['series_id'].unique():
        series_forecast = forecast_df[forecast_df['series_id'] == series_id].copy()
        
        if actual_df is None:
            continue
        
        series_actual = actual_df[actual_df['series_id'] == series_id].copy()
        
        # ê¸°ê°„ ë§¤ì¹­
        merged = pd.merge(
            series_forecast[['week_end_date', 'yhat']],
            series_actual[['week_end_date', 'y']],
            on='week_end_date',
            how='inner'
        )
        
        if len(merged) < 10:
            continue
        
        # ì”ì°¨ ê³„ì‚°
        residuals = merged['y'] - merged['yhat']
        
        # 1. Ljung-Box í…ŒìŠ¤íŠ¸ (ì”ì°¨ì˜ ìê¸°ìƒê´€)
        try:
            lb_test = acorr_ljungbox(residuals, lags=[10], return_df=True)
            lb_pvalue = lb_test['lb_pvalue'].values[0]
        except:
            lb_pvalue = np.nan
        
        # 2. ì •ê·œì„± í…ŒìŠ¤íŠ¸ (Shapiro-Wilk)
        try:
            sw_stat, sw_pvalue = stats.shapiro(residuals)
        except:
            sw_stat, sw_pvalue = np.nan, np.nan
        
        # 3. ê¸°ë³¸ í†µê³„
        residual_mean = residuals.mean()
        residual_std = residuals.std()
        residual_skew = stats.skew(residuals)
        residual_kurt = stats.kurtosis(residuals)
        
        results.append({
            'series_id': series_id,
            'n_residuals': len(residuals),
            'ljungbox_pvalue': lb_pvalue,
            'ljungbox_pass': lb_pvalue > 0.05 if not np.isnan(lb_pvalue) else None,
            'shapiro_pvalue': sw_pvalue,
            'normality_pass': sw_pvalue > 0.05 if not np.isnan(sw_pvalue) else None,
            'residual_mean': residual_mean,
            'residual_std': residual_std,
            'residual_skew': residual_skew,
            'residual_kurtosis': residual_kurt,
        })
    
    return pd.DataFrame(results)


def compute_baseline_metrics(forecast_df: pd.DataFrame, 
                            actual_df: pd.DataFrame,
                            train_df: pd.DataFrame = None) -> pd.DataFrame:
    """
    ê¸°ì¤€ì„  ë©”íŠ¸ë¦­ ê³„ì‚°
    
    Args:
        forecast_df: ì˜ˆì¸¡ ë°ì´í„°í”„ë ˆì„
        actual_df: ì‹¤ì¸¡ ë°ì´í„°í”„ë ˆì„
        train_df: í•™ìŠµ ë°ì´í„°í”„ë ˆì„ (MASEìš©)
    
    Returns:
        ë©”íŠ¸ë¦­ ë°ì´í„°í”„ë ˆì„
    """
    if actual_df is None:
        print("âš ï¸  ì‹¤ì¸¡ê°’ì´ ì—†ì–´ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    # ì˜ˆì¸¡-ì‹¤ì¸¡ ë³‘í•©
    merged = pd.merge(
        forecast_df[['series_id', 'week_end_date', 'yhat', 'model_type']],
        actual_df[['series_id', 'week_end_date', 'y']],
        on=['series_id', 'week_end_date'],
        how='inner'
    )
    
    if len(merged) == 0:
        print("âš ï¸  ë§¤ì¹­ë˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    metrics_df = compute_metrics_by_group(
        df=merged,
        y_true_col='y',
        y_pred_col='yhat',
        group_cols=['series_id'],
        y_train=train_df,
        train_group_col='series_id'
    )
    
    # ëª¨ë¸ íƒ€ì… ì¶”ê°€
    model_types = merged.groupby('series_id')['model_type'].first()
    metrics_df = metrics_df.merge(
        model_types.reset_index(),
        on='series_id',
        how='left'
    )
    
    return metrics_df


def analyze_fallback_rate(forecast_df: pd.DataFrame) -> pd.DataFrame:
    """
    í´ë°± ëª¨ë¸ ì‚¬ìš©ë¥  ë¶„ì„
    
    Args:
        forecast_df: ì˜ˆì¸¡ ë°ì´í„°í”„ë ˆì„
    
    Returns:
        í´ë°± ì‚¬ìš©ë¥  ë°ì´í„°í”„ë ˆì„
    """
    if 'model_type' not in forecast_df.columns:
        print("âš ï¸  model_type ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    # ì‹œë¦¬ì¦ˆë³„ ëª¨ë¸ íƒ€ì… ì§‘ê³„
    fallback_summary = forecast_df.groupby('series_id').agg({
        'model_type': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'unknown'
    }).reset_index()
    
    fallback_summary.columns = ['series_id', 'primary_model']
    
    # ì „ì²´ í†µê³„
    total_series = len(fallback_summary)
    fallback_counts = fallback_summary['primary_model'].value_counts()
    
    fallback_stats = pd.DataFrame({
        'model_type': fallback_counts.index,
        'count': fallback_counts.values,
        'percentage': (fallback_counts.values / total_series * 100).round(2)
    })
    
    return fallback_summary, fallback_stats


def generate_report(residual_df: pd.DataFrame,
                   metrics_df: pd.DataFrame,
                   fallback_summary: pd.DataFrame,
                   fallback_stats: pd.DataFrame,
                   output_path: Path,
                   year: int):
    """
    ì¢…í•© ë³´ê³ ì„œ ìƒì„±
    
    Args:
        residual_df: ì”ì°¨ ë¶„ì„ ê²°ê³¼
        metrics_df: ë©”íŠ¸ë¦­ ê²°ê³¼
        fallback_summary: í´ë°± ìš”ì•½
        fallback_stats: í´ë°± í†µê³„
        output_path: ì¶œë ¥ ê²½ë¡œ
        year: ê²€ì¦ ì—°ë„
    """
    output_path.mkdir(parents=True, exist_ok=True)
    report_file = output_path / f'baseline_report_{year}.md'
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f"# Baseline ê²€ì¦ ë³´ê³ ì„œ - {year}ë…„\n\n")
        f.write(f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("---\n\n")
        
        # 1. ì „ì²´ ìš”ì•½
        f.write("## ğŸ“Š ì „ì²´ ìš”ì•½\n\n")
        if not metrics_df.empty:
            f.write(f"- **ì´ ì‹œë¦¬ì¦ˆ ìˆ˜**: {len(metrics_df)}\n")
            f.write(f"- **í‰ê·  MAPE**: {metrics_df['mape'].mean():.4f}\n")
            f.write(f"- **í‰ê·  Bias**: {metrics_df['bias'].mean():.4f}\n")
            if 'mase' in metrics_df.columns:
                f.write(f"- **í‰ê·  MASE**: {metrics_df['mase'].mean():.4f}\n")
            f.write(f"- **í‰ê·  MAE**: {metrics_df['mae'].mean():.2f}\n")
            f.write(f"- **í‰ê·  RMSE**: {metrics_df['rmse'].mean():.2f}\n\n")
        
        # 2. í´ë°± ëª¨ë¸ í†µê³„
        f.write("## ğŸ”„ í´ë°± ëª¨ë¸ ì‚¬ìš©ë¥ \n\n")
        if not fallback_stats.empty:
            f.write("| ëª¨ë¸ íƒ€ì… | ì‹œë¦¬ì¦ˆ ìˆ˜ | ë¹„ìœ¨ (%) |\n")
            f.write("|----------|----------|----------|\n")
            for _, row in fallback_stats.iterrows():
                f.write(f"| {row['model_type']} | {row['count']} | {row['percentage']:.2f}% |\n")
            f.write("\n")
        
        # 3. ì”ì°¨ ì§„ë‹¨
        f.write("## ğŸ” ì”ì°¨ ì§„ë‹¨\n\n")
        if not residual_df.empty:
            ljung_pass = residual_df['ljungbox_pass'].sum()
            ljung_total = residual_df['ljungbox_pass'].notna().sum()
            norm_pass = residual_df['normality_pass'].sum()
            norm_total = residual_df['normality_pass'].notna().sum()
            
            f.write(f"- **Ljung-Box í…ŒìŠ¤íŠ¸ í†µê³¼ìœ¨**: {ljung_pass}/{ljung_total} ({ljung_pass/ljung_total*100:.1f}%)\n")
            f.write(f"- **ì •ê·œì„± í…ŒìŠ¤íŠ¸ í†µê³¼ìœ¨**: {norm_pass}/{norm_total} ({norm_pass/norm_total*100:.1f}%)\n\n")
            
            # ë¬¸ì œ ì‹œë¦¬ì¦ˆ
            failed = residual_df[
                (residual_df['ljungbox_pass'] == False) | 
                (residual_df['normality_pass'] == False)
            ]
            
            if len(failed) > 0:
                f.write("### âš ï¸ ì”ì°¨ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì‹œë¦¬ì¦ˆ\n\n")
                f.write(f"ì´ {len(failed)}ê°œ ì‹œë¦¬ì¦ˆ\n\n")
        
        # 4. ì„±ëŠ¥ ë¬¸ì œ ì‹œë¦¬ì¦ˆ
        f.write("## ğŸ¯ íŠœë‹ í›„ë³´ ì‹œë¦¬ì¦ˆ\n\n")
        if not metrics_df.empty:
            candidates = identify_poor_performers(
                metrics_df,
                mape_threshold=0.20,
                bias_threshold=0.05,
                mase_threshold=1.5
            )
            
            f.write(f"**MAPE>0.20 ë˜ëŠ” Bias>0.05 ë˜ëŠ” MASE>1.5 ì‹œë¦¬ì¦ˆ**: {len(candidates)}ê°œ\n\n")
            
            if len(candidates) > 0:
                f.write("### Top 20 ìš°ì„ ìˆœìœ„ ì‹œë¦¬ì¦ˆ\n\n")
                f.write("| Series ID | MAPE | Bias | MASE | ìš°ì„ ìˆœìœ„ |\n")
                f.write("|-----------|------|------|------|----------|\n")
                
                for _, row in candidates.head(20).iterrows():
                    mase_val = f"{row['mase']:.3f}" if 'mase' in row and not pd.isna(row['mase']) else 'N/A'
                    f.write(f"| {row['series_id']} | {row['mape']:.4f} | {row['bias']:.4f} | {mase_val} | {row['priority_score']:.2f} |\n")
                
                f.write("\n")
                
                # íŠœë‹ í›„ë³´ CSV ì €ì¥
                candidates_path = output_path.parent / 'metrics' / f'tuning_candidates_{year}.csv'
                candidates_path.parent.mkdir(parents=True, exist_ok=True)
                candidates.to_csv(candidates_path, index=False)
                f.write(f"ğŸ“ ì „ì²´ íŠœë‹ í›„ë³´ ëª©ë¡: `{candidates_path.relative_to(project_root)}`\n\n")
        
        # 5. ëª¨ë¸ íƒ€ì…ë³„ ì„±ëŠ¥
        f.write("## ğŸ“ˆ ëª¨ë¸ íƒ€ì…ë³„ ì„±ëŠ¥\n\n")
        if not metrics_df.empty and 'model_type' in metrics_df.columns:
            model_perf = metrics_df.groupby('model_type').agg({
                'mape': 'mean',
                'bias': 'mean',
                'mae': 'mean',
                'series_id': 'count'
            }).round(4)
            model_perf.columns = ['í‰ê· _MAPE', 'í‰ê· _Bias', 'í‰ê· _MAE', 'ì‹œë¦¬ì¦ˆ_ìˆ˜']
            
            f.write(model_perf.to_markdown())
            f.write("\n\n")
        
        f.write("---\n\n")
        f.write("ë‹¤ìŒ ë‹¨ê³„: Rolling ë°±í…ŒìŠ¤íŠ¸ë¡œ ê¸°ì¤€ì„  í™•ë¦½\n")
        f.write("```bash\n")
        f.write("python batch.py roll --start 2020 --end 2024\n")
        f.write("```\n")
    
    print(f"\nâœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_file}")


def main():
    parser = argparse.ArgumentParser(description='Baseline í•™ìŠµ ê²°ê³¼ ê²€ì¦')
    parser.add_argument('--year', type=int, default=2024, help='ê²€ì¦ ì—°ë„')
    parser.add_argument('--artifacts', type=str, default='artifacts', help='ì•„í‹°íŒ©íŠ¸ ë””ë ‰í† ë¦¬')
    parser.add_argument('--output', type=str, default='reports', help='ë³´ê³ ì„œ ì¶œë ¥ ë””ë ‰í† ë¦¬')
    
    args = parser.parse_args()
    
    artifacts_path = Path(args.artifacts)
    output_path = Path(args.output)
    
    print(f"\n{'='*60}")
    print(f"Baseline ê²€ì¦ - {args.year}ë…„")
    print(f"{'='*60}\n")
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    forecast_df, actual_df = load_forecast_results(artifacts_path, args.year)
    print(f"   ì˜ˆì¸¡ ë°ì´í„°: {len(forecast_df)} í–‰")
    if actual_df is not None:
        print(f"   ì‹¤ì¸¡ ë°ì´í„°: {len(actual_df)} í–‰")
    
    # í•™ìŠµ ë°ì´í„° ë¡œë“œ (MASE ê³„ì‚°ìš©)
    train_path = project_root / 'data' / 'curated' / f'curated_{args.year - 1}.parquet'
    train_df = pd.read_parquet(train_path) if train_path.exists() else None
    
    # 2. ì”ì°¨ ë¶„ì„
    print("\nğŸ” ì”ì°¨ ë¶„ì„ ì¤‘...")
    residual_df = analyze_residuals(forecast_df, actual_df)
    print(f"   ë¶„ì„ ì™„ë£Œ: {len(residual_df)} ì‹œë¦¬ì¦ˆ")
    
    # 3. ë©”íŠ¸ë¦­ ê³„ì‚°
    print("\nğŸ“Š ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
    metrics_df = compute_baseline_metrics(forecast_df, actual_df, train_df)
    print(f"   ê³„ì‚° ì™„ë£Œ: {len(metrics_df)} ì‹œë¦¬ì¦ˆ")
    
    # 4. í´ë°± ë¶„ì„
    print("\nğŸ”„ í´ë°± ëª¨ë¸ ë¶„ì„ ì¤‘...")
    fallback_summary, fallback_stats = analyze_fallback_rate(forecast_df)
    print(f"   ë¶„ì„ ì™„ë£Œ")
    
    # 5. ë©”íŠ¸ë¦­ ì €ì¥
    print("\nğŸ’¾ ë©”íŠ¸ë¦­ ì €ì¥ ì¤‘...")
    metrics_path = artifacts_path / 'metrics'
    metrics_path.mkdir(parents=True, exist_ok=True)
    
    if not metrics_df.empty:
        metrics_file = metrics_path / f'metrics_baseline_{args.year}.parquet'
        metrics_df.to_parquet(metrics_file, index=False)
        print(f"   âœ“ {metrics_file}")
    
    if not residual_df.empty:
        residual_file = metrics_path / f'residual_analysis_{args.year}.parquet'
        residual_df.to_parquet(residual_file, index=False)
        print(f"   âœ“ {residual_file}")
    
    if not fallback_summary.empty:
        fallback_file = metrics_path / f'fallback_summary_{args.year}.csv'
        fallback_summary.to_csv(fallback_file, index=False)
        print(f"   âœ“ {fallback_file}")
    
    # 6. ë³´ê³ ì„œ ìƒì„±
    print("\nğŸ“ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    generate_report(
        residual_df, metrics_df, fallback_summary, fallback_stats,
        output_path, args.year
    )
    
    # 7. ìš”ì•½ ì¶œë ¥
    print(f"\n{'='*60}")
    print("ìš”ì•½")
    print(f"{'='*60}")
    
    if not metrics_df.empty:
        print(f"\nì´ ì‹œë¦¬ì¦ˆ: {len(metrics_df)}")
        print(f"í‰ê·  MAPE: {metrics_df['mape'].mean():.4f}")
        print(f"í‰ê·  Bias: {metrics_df['bias'].mean():.4f}")
        
        candidates = identify_poor_performers(metrics_df)
        print(f"\níŠœë‹ í›„ë³´ ì‹œë¦¬ì¦ˆ: {len(candidates)}ê°œ ({len(candidates)/len(metrics_df)*100:.1f}%)")
    
    if not fallback_stats.empty:
        print("\ní´ë°± ëª¨ë¸ ì‚¬ìš©ë¥ :")
        for _, row in fallback_stats.iterrows():
            print(f"  {row['model_type']}: {row['percentage']:.1f}%")
    
    print(f"\n{'='*60}\n")


if __name__ == '__main__':
    main()

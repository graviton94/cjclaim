"""
ì—°ë„ë³„ ë¶„í•  Raw Data ë³‘í•© ìŠ¤í¬ë¦½íŠ¸
====================================
C:\cjclaim\data ì—ì„œ ë…„ë„ë³„ CSV íŒŒì¼ë“¤ì„ ì½ì–´ì„œ
quality-cycles\data\raw ì— í†µí•© íŒŒì¼ë¡œ ì €ì¥

ì‚¬ìš©ë²•:
    python merge_yearly_data.py
    python merge_yearly_data.py --source C:\cjclaim\data --years 2021 2022 2023 2024
    python merge_yearly_data.py --pattern "claims_*.csv"
"""
import pandas as pd
from pathlib import Path
import argparse
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')


def detect_encoding(file_path):
    """CSV íŒŒì¼ ì¸ì½”ë”© ìë™ íƒì§€"""
    encodings = ['utf-8-sig', 'utf-8', 'euc-kr', 'cp949']
    
    for enc in encodings:
        try:
            pd.read_csv(file_path, encoding=enc, nrows=5)
            return enc
        except:
            continue
    
    return 'utf-8'  # fallback


def load_yearly_file(file_path, encoding=None):
    """ë‹¨ì¼ ì—°ë„ íŒŒì¼ ë¡œë“œ"""
    if encoding is None:
        encoding = detect_encoding(file_path)
    
    print(f"  Loading {file_path.name} (encoding: {encoding})...")
    
    try:
        df = pd.read_csv(file_path, encoding=encoding)
        print(f"    â†’ {len(df):,} rows loaded")
        return df, encoding
    except Exception as e:
        print(f"    âœ— Error: {e}")
        return None, encoding


def merge_yearly_data(
    source_dir: str = "C:/cjclaim/data",
    output_path: str = "data/raw/claims_merged.csv",
    years: list = None,
    file_pattern: str = None,
    deduplicate: bool = True
):
    """
    ì—¬ëŸ¬ ì—°ë„ë³„ íŒŒì¼ì„ ë³‘í•©
    
    Args:
        source_dir: ì›ë³¸ ë°ì´í„° ë””ë ‰í† ë¦¬
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        years: ë³‘í•©í•  ì—°ë„ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ìë™ íƒì§€)
        file_pattern: íŒŒì¼ëª… íŒ¨í„´ (e.g., "claims_*.csv")
        deduplicate: ì¤‘ë³µ ì œê±° ì—¬ë¶€
    
    Returns:
        ë³‘í•©ëœ DataFrame
    """
    source_path = Path(source_dir)
    
    if not source_path.exists():
        raise FileNotFoundError(f"Source directory not found: {source_dir}")
    
    print("=" * 80)
    print("Yearly Data Merge Pipeline")
    print("=" * 80)
    print(f"Source directory: {source_path}")
    print(f"Output path: {output_path}")
    print()
    
    # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    if file_pattern:
        # íŒ¨í„´ ê¸°ë°˜
        files = sorted(source_path.glob(file_pattern))
    elif years:
        # ì—°ë„ ê¸°ë°˜
        files = []
        for year in years:
            # ì—¬ëŸ¬ ê°€ëŠ¥í•œ íŒŒì¼ëª… íŒ¨í„´ ì‹œë„
            patterns = [
                f"{year}_raw.csv",
                f"claims_{year}.csv",
                f"{year}.csv",
                f"data_{year}.csv"
            ]
            for pattern in patterns:
                matches = list(source_path.glob(pattern))
                if matches:
                    files.extend(matches)
                    break
    else:
        # ëª¨ë“  CSV íŒŒì¼
        files = sorted(source_path.glob("*.csv"))
    
    if not files:
        raise FileNotFoundError(f"No CSV files found in {source_dir}")
    
    print(f"Found {len(files)} file(s) to merge:")
    for f in files:
        print(f"  - {f.name}")
    print()
    
    # ìˆœì°¨ì ìœ¼ë¡œ ë¡œë“œ ë° ë³‘í•©
    all_dfs = []
    total_rows = 0
    encoding_used = None
    
    for i, file_path in enumerate(files, 1):
        print(f"[{i}/{len(files)}]", end=" ")
        df, enc = load_yearly_file(file_path, encoding_used)
        
        if df is not None:
            all_dfs.append(df)
            total_rows += len(df)
            encoding_used = enc  # ê°™ì€ ì¸ì½”ë”© ì¬ì‚¬ìš©
    
    if not all_dfs:
        raise ValueError("No data loaded successfully")
    
    # ë³‘í•©
    print(f"\n{'='*80}")
    print(f"Merging {len(all_dfs)} DataFrames...")
    df_merged = pd.concat(all_dfs, ignore_index=True)
    
    print(f"  Total rows before processing: {len(df_merged):,}")
    
    # ì¤‘ë³µ ì²˜ë¦¬: drop_duplicates ëŒ€ì‹  groupbyë¡œ count í•©ì‚°
    if deduplicate:
        original_len = len(df_merged)
        
        # ê·¸ë£¹í™” í‚¤ ê²°ì • (ë°œìƒì¼ì, ì¤‘ë¶„ë¥˜, í”ŒëœíŠ¸, ì œí’ˆë²”ì£¼2, ì œì¡°ì¼ì)
        group_cols = []
        for col in ['ë°œìƒì¼ì', 'ì¤‘ë¶„ë¥˜', 'í”ŒëœíŠ¸', 'ì œí’ˆë²”ì£¼2', 'ì œì¡°ì¼ì']:
            if col in df_merged.columns:
                group_cols.append(col)
        
        # count ì»¬ëŸ¼ í™•ì¸
        count_col = None
        for col in ['count', 'claim_count', 'y']:
            if col in df_merged.columns:
                count_col = col
                break
        
        if len(group_cols) >= 4 and count_col:
            # ê°™ì€ í‚¤ì— ëŒ€í•´ count ê°’ì„ í•©ì‚° (ì¤‘ë³µì´ ì•„ë‹ˆë¼ ì‹¤ì œ ë°œìƒ ê±´ìˆ˜)
            df_merged = df_merged.groupby(group_cols, as_index=False)[count_col].sum()
            new_len = len(df_merged)
            print(f"  Aggregated {original_len:,} rows â†’ {new_len:,} unique series-date combinations")
            print(f"  (Same series+date occurrences were summed, not removed)")
        else:
            print(f"  âš ï¸  Insufficient columns for aggregation, keeping all rows")
    
    print(f"  Final rows: {len(df_merged):,}")
    
    # ë°ì´í„° í’ˆì§ˆ ì²´í¬
    print(f"\n{'='*80}")
    print("Data Quality Check")
    print(f"{'='*80}")
    print(f"Columns: {len(df_merged.columns)}")
    print(f"  {', '.join(df_merged.columns[:10].tolist())}...")
    
    # ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸
    date_col = None
    for col in ['ì œì¡°ì¼ì', 'date', 'ë°œìƒì¼', 'occurrence_date']:
        if col in df_merged.columns:
            date_col = col
            break
    
    if date_col:
        # ë‚ ì§œ íŒŒì‹± ì‹œë„
        try:
            df_merged[date_col] = pd.to_datetime(df_merged[date_col], errors='coerce')
            df_merged['year'] = df_merged[date_col].dt.year
            df_merged['month'] = df_merged[date_col].dt.month
            
            year_counts = df_merged['year'].value_counts().sort_index()
            print(f"\nYear distribution:")
            for year, count in year_counts.items():
                if pd.notna(year):
                    print(f"  {int(year)}: {count:,} rows")
        except:
            print(f"  Warning: Could not parse date column '{date_col}'")
    
    # ì €ì¥
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*80}")
    print(f"Saving to: {output_path}")
    df_merged.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    file_size_mb = output_path.stat().st_size / (1024 * 1024)
    print(f"  File size: {file_size_mb:.2f} MB")
    print(f"  Rows: {len(df_merged):,}")
    print(f"  Columns: {len(df_merged.columns)}")
    
    print(f"\nâœ… SUCCESS: Merged data saved!")
    print(f"{'='*80}")
    
    return df_merged


def main():
    parser = argparse.ArgumentParser(
        description="Merge yearly CSV files into single dataset",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Auto-detect all CSV files in C:\cjclaim\data
  python merge_yearly_data.py
  
  # Specify years explicitly
  python merge_yearly_data.py --years 2021 2022 2023 2024
  
  # Use file pattern
  python merge_yearly_data.py --pattern "claims_*.csv"
  
  # Custom source directory
  python merge_yearly_data.py --source D:\backup\claims_data
  
  # No deduplication
  python merge_yearly_data.py --no-deduplicate
        """
    )
    
    parser.add_argument("--source", type=str, default="C:/cjclaim/data",
                        help="Source directory containing yearly CSV files (default: C:/cjclaim/data)")
    parser.add_argument("--output", type=str, default="data/raw/claims_merged.csv",
                        help="Output merged CSV path (default: data/raw/claims_merged.csv)")
    parser.add_argument("--years", type=int, nargs='+',
                        help="Specific years to merge (e.g., --years 2021 2022 2023)")
    parser.add_argument("--pattern", type=str,
                        help="File name pattern (e.g., --pattern 'claims_*.csv')")
    parser.add_argument("--no-deduplicate", action="store_true",
                        help="Skip duplicate removal")
    
    args = parser.parse_args()
    
    try:
        df_merged = merge_yearly_data(
            source_dir=args.source,
            output_path=args.output,
            years=args.years,
            file_pattern=args.pattern,
            deduplicate=not args.no_deduplicate
        )
        
        # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
        print(f"\nğŸ“‹ Next Steps:")
        print(f"1. Verify data: head data/raw/claims_merged.csv")
        print(f"2. Preprocess: python preprocess_to_curated.py --input {args.output}")
        print(f"3. Generate JSONs: python generate_series_json.py")
        print(f"4. Train models: python train_base_models.py --auto-optimize")
        
    except Exception as e:
        print(f"\nâŒ ERROR: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())

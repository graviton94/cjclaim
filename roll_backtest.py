"""
Rolling ë°±í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸

ì—°ë„ë³„ë¡œ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ-ì˜ˆì¸¡-í‰ê°€ë¥¼ ë°˜ë³µí•˜ì—¬
ëª¨ë¸ì˜ ì‹œê°„ì— ë”°ë¥¸ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  íŠœë‹ í›„ë³´ë¥¼ ì„ ë³„

ì‹¤í–‰ ì˜ˆì‹œ:
    python roll_backtest.py --start 2020 --end 2024
    python roll_backtest.py --start 2020 --end 2024 --series "series_123"
"""

import sys
import argparse
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime
from typing import List, Dict
import warnings

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from pipeline_train import train_until
from pipeline_forecast import forecast_year
from src.metrics import compute_metrics_by_group, identify_poor_performers

warnings.filterwarnings('ignore')


def run_rolling_backtest(curated_path: Path,
                         start_year: int,
                         end_year: int,
                         series_filter: str = "all",
                         engine: str = "pandas") -> pd.DataFrame:
    """
    Rolling ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    ê° ì—°ë„ì— ëŒ€í•´:
    1. í•´ë‹¹ ì—°ë„ê¹Œì§€ì˜ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ
    2. ë‹¤ìŒ ì—°ë„ ì˜ˆì¸¡
    3. ì‹¤ì¸¡ê°’ê³¼ ë¹„êµí•˜ì—¬ ë©”íŠ¸ë¦­ ê³„ì‚°
    
    Args:
        curated_path: Curated ë°ì´í„° ê²½ë¡œ
        start_year: ì‹œì‘ ì—°ë„
        end_year: ì¢…ë£Œ ì—°ë„ (ì´ ì—°ë„ê¹Œì§€ í•™ìŠµ, ë‹¤ìŒ ì—°ë„ ì˜ˆì¸¡)
        series_filter: ì‹œë¦¬ì¦ˆ í•„í„°
        engine: ì‹¤í–‰ ì—”ì§„
    
    Returns:
        ì—°ë„ë³„ ë©”íŠ¸ë¦­ ë°ì´í„°í”„ë ˆì„
    """
    all_metrics = []
    
    print(f"\n{'='*70}")
    print(f"Rolling ë°±í…ŒìŠ¤íŠ¸: {start_year} ~ {end_year}")
    print(f"{'='*70}\n")
    
    # ì „ì²´ ë°ì´í„° ë¡œë“œ
    if curated_path.exists():
        full_data = pd.read_parquet(curated_path)
    else:
        raise FileNotFoundError(f"Curated ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {curated_path}")
    
    for train_year in range(start_year, end_year + 1):
        test_year = train_year + 1
        
        print(f"\n{'='*70}")
        print(f"[{train_year}ë…„ í•™ìŠµ â†’ {test_year}ë…„ ì˜ˆì¸¡]")
        print(f"{'='*70}\n")
        
        # 1. í•™ìŠµ
        print(f"ğŸ“š {train_year}ë…„ê¹Œì§€ ë°ì´í„°ë¡œ í•™ìŠµ ì¤‘...")
        try:
            train_until(curated_path, train_year)
            print(f"   âœ“ í•™ìŠµ ì™„ë£Œ")
        except Exception as e:
            print(f"   âœ— í•™ìŠµ ì‹¤íŒ¨: {e}")
            continue
        
        # 2. ì˜ˆì¸¡
        print(f"\nğŸ”® {test_year}ë…„ ì˜ˆì¸¡ ì¤‘...")
        try:
            forecast_year(curated_path, test_year)
            print(f"   âœ“ ì˜ˆì¸¡ ì™„ë£Œ")
        except Exception as e:
            print(f"   âœ— ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            continue
        
        # 3. í‰ê°€
        print(f"\nğŸ“Š {test_year}ë…„ ì‹¤ì¸¡ê°’ê³¼ ë¹„êµ ì¤‘...")
        
        # ì˜ˆì¸¡ ê²°ê³¼ ë¡œë“œ
        forecast_path = Path('artifacts/forecasts') / f'forecast_{test_year}.parquet'
        if not forecast_path.exists():
            print(f"   âš ï¸  ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤: {forecast_path}")
            continue
        
        forecast_df = pd.read_parquet(forecast_path)
        
        # ì‹¤ì¸¡ê°’ í•„í„°ë§
        actual_df = full_data[full_data['year'] == test_year].copy()
        
        if len(actual_df) == 0:
            print(f"   âš ï¸  {test_year}ë…„ ì‹¤ì¸¡ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
            continue
        
        # ì˜ˆì¸¡-ì‹¤ì¸¡ ë³‘í•©
        merged = pd.merge(
            forecast_df[['series_id', 'week_end_date', 'yhat', 'model_type']],
            actual_df[['series_id', 'week_end_date', 'y']],
            on=['series_id', 'week_end_date'],
            how='inner'
        )
        
        if len(merged) == 0:
            print(f"   âš ï¸  ë§¤ì¹­ë˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue
        
        # í•™ìŠµ ë°ì´í„° (MASE ê³„ì‚°ìš©)
        train_df = full_data[full_data['year'] <= train_year].copy()
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        year_metrics = compute_metrics_by_group(
            df=merged,
            y_true_col='y',
            y_pred_col='yhat',
            group_cols=['series_id'],
            y_train=train_df,
            train_group_col='series_id'
        )
        
        # ì—°ë„ ì •ë³´ ì¶”ê°€
        year_metrics['train_year'] = train_year
        year_metrics['test_year'] = test_year
        
        # ëª¨ë¸ íƒ€ì… ì¶”ê°€
        model_types = merged.groupby('series_id')['model_type'].first()
        year_metrics = year_metrics.merge(
            model_types.reset_index(),
            on='series_id',
            how='left'
        )
        
        all_metrics.append(year_metrics)
        
        print(f"   âœ“ í‰ê°€ ì™„ë£Œ: {len(year_metrics)} ì‹œë¦¬ì¦ˆ")
        print(f"   - í‰ê·  MAPE: {year_metrics['mape'].mean():.4f}")
        print(f"   - í‰ê·  Bias: {year_metrics['bias'].mean():.4f}")
        if 'mase' in year_metrics.columns:
            print(f"   - í‰ê·  MASE: {year_metrics['mase'].mean():.4f}")
    
    # ì „ì²´ ê²°ê³¼ ë³‘í•©
    if len(all_metrics) == 0:
        print("\nâš ï¸  ë©”íŠ¸ë¦­ì´ ê³„ì‚°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    all_metrics_df = pd.concat(all_metrics, ignore_index=True)
    
    return all_metrics_df


def analyze_trends(metrics_df: pd.DataFrame) -> pd.DataFrame:
    """
    ì‹œê³„ì—´ë³„ ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
    
    Args:
        metrics_df: Rolling ë°±í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­
    
    Returns:
        íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼
    """
    if metrics_df.empty:
        return pd.DataFrame()
    
    trends = []
    
    for series_id in metrics_df['series_id'].unique():
        series_data = metrics_df[metrics_df['series_id'] == series_id].sort_values('test_year')
        
        if len(series_data) < 2:
            continue
        
        # ì„±ëŠ¥ ë³€í™” ê³„ì‚°
        mape_trend = series_data['mape'].diff().mean()  # í‰ê·  ë³€í™”ìœ¨
        bias_trend = series_data['bias'].diff().mean()
        
        # ìµœê·¼ ì„±ëŠ¥
        recent_mape = series_data['mape'].iloc[-1]
        recent_bias = series_data['bias'].iloc[-1]
        
        # í‰ê·  ì„±ëŠ¥
        avg_mape = series_data['mape'].mean()
        avg_bias = series_data['bias'].mean()
        
        # ì„±ëŠ¥ ì•…í™” ì—¬ë¶€
        is_degrading = mape_trend > 0.01  # MAPEê°€ ì¦ê°€ ì¶”ì„¸
        
        trends.append({
            'series_id': series_id,
            'n_years': len(series_data),
            'avg_mape': avg_mape,
            'avg_bias': avg_bias,
            'recent_mape': recent_mape,
            'recent_bias': recent_bias,
            'mape_trend': mape_trend,
            'bias_trend': bias_trend,
            'is_degrading': is_degrading,
        })
    
    return pd.DataFrame(trends)


def generate_rolling_report(metrics_df: pd.DataFrame,
                            trends_df: pd.DataFrame,
                            start_year: int,
                            end_year: int,
                            output_path: Path):
    """
    Rolling ë°±í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±
    
    Args:
        metrics_df: ë©”íŠ¸ë¦­ ë°ì´í„°í”„ë ˆì„
        trends_df: íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼
        start_year: ì‹œì‘ ì—°ë„
        end_year: ì¢…ë£Œ ì—°ë„
        output_path: ì¶œë ¥ ê²½ë¡œ
    """
    output_path.mkdir(parents=True, exist_ok=True)
    report_file = output_path / f'rolling_backtest_{start_year}_{end_year}.md'
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f"# Rolling ë°±í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ\n\n")
        f.write(f"**ê¸°ê°„**: {start_year} ~ {end_year}\n")
        f.write(f"**ìƒì„±ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("---\n\n")
        
        # 1. ì—°ë„ë³„ ì „ì²´ ì„±ëŠ¥
        f.write("## ğŸ“Š ì—°ë„ë³„ ì „ì²´ ì„±ëŠ¥\n\n")
        if not metrics_df.empty:
            yearly_summary = metrics_df.groupby('test_year').agg({
                'mape': 'mean',
                'bias': 'mean',
                'mae': 'mean',
                'series_id': 'count'
            }).round(4)
            yearly_summary.columns = ['í‰ê· _MAPE', 'í‰ê· _Bias', 'í‰ê· _MAE', 'ì‹œë¦¬ì¦ˆ_ìˆ˜']
            
            f.write(yearly_summary.to_markdown())
            f.write("\n\n")
        
        # 2. ëª¨ë¸ íƒ€ì…ë³„ ì„±ëŠ¥
        f.write("## ğŸ”§ ëª¨ë¸ íƒ€ì…ë³„ ì„±ëŠ¥ (ì „ì²´ ê¸°ê°„)\n\n")
        if 'model_type' in metrics_df.columns:
            model_summary = metrics_df.groupby('model_type').agg({
                'mape': 'mean',
                'bias': 'mean',
                'series_id': 'count'
            }).round(4)
            model_summary.columns = ['í‰ê· _MAPE', 'í‰ê· _Bias', 'ì‹œë¦¬ì¦ˆ_ìˆ˜']
            
            f.write(model_summary.to_markdown())
            f.write("\n\n")
        
        # 3. ì„±ëŠ¥ ì•…í™” ì‹œë¦¬ì¦ˆ
        f.write("## âš ï¸ ì„±ëŠ¥ ì•…í™” ì¶”ì„¸ ì‹œë¦¬ì¦ˆ\n\n")
        if not trends_df.empty:
            degrading = trends_df[trends_df['is_degrading'] == True].sort_values(
                'mape_trend', ascending=False
            )
            
            f.write(f"**ì´ {len(degrading)}ê°œ ì‹œë¦¬ì¦ˆ**ì—ì„œ ì„±ëŠ¥ ì•…í™” ì¶”ì„¸ ê°ì§€\n\n")
            
            if len(degrading) > 0:
                f.write("### Top 20 ì•…í™” ì‹œë¦¬ì¦ˆ\n\n")
                f.write("| Series ID | í‰ê·  MAPE | ìµœê·¼ MAPE | MAPE ì¦ê°€ìœ¨ | Bias ë³€í™” |\n")
                f.write("|-----------|-----------|-----------|-------------|----------|\n")
                
                for _, row in degrading.head(20).iterrows():
                    f.write(f"| {row['series_id']} | {row['avg_mape']:.4f} | {row['recent_mape']:.4f} | {row['mape_trend']:+.4f} | {row['bias_trend']:+.4f} |\n")
                
                f.write("\n")
        
        # 4. íŠœë‹ í›„ë³´ ì‹œë¦¬ì¦ˆ (ìµœì¢… ì—°ë„ ê¸°ì¤€)
        f.write("## ğŸ¯ íŠœë‹ í›„ë³´ ì‹œë¦¬ì¦ˆ (ìµœì¢… ì—°ë„ ê¸°ì¤€)\n\n")
        if not metrics_df.empty:
            latest_year = metrics_df['test_year'].max()
            latest_metrics = metrics_df[metrics_df['test_year'] == latest_year]
            
            candidates = identify_poor_performers(
                latest_metrics,
                mape_threshold=0.20,
                bias_threshold=0.05,
                mase_threshold=1.5
            )
            
            f.write(f"**{latest_year}ë…„ ê¸°ì¤€**: {len(candidates)}ê°œ ì‹œë¦¬ì¦ˆê°€ íŠœë‹ í•„ìš”\n\n")
            
            if len(candidates) > 0:
                f.write("### Top 20 ìš°ì„ ìˆœìœ„ ì‹œë¦¬ì¦ˆ\n\n")
                f.write("| Series ID | MAPE | Bias | MASE | ìš°ì„ ìˆœìœ„ |\n")
                f.write("|-----------|------|------|------|----------|\n")
                
                for _, row in candidates.head(20).iterrows():
                    mase_val = f"{row['mase']:.3f}" if 'mase' in row and not pd.isna(row['mase']) else 'N/A'
                    f.write(f"| {row['series_id']} | {row['mape']:.4f} | {row['bias']:.4f} | {mase_val} | {row['priority_score']:.2f} |\n")
                
                f.write("\n")
                
                # íŠœë‹ í›„ë³´ CSV ì €ì¥
                candidates_path = Path('artifacts/metrics') / f'tuning_candidates_rolling_{start_year}_{end_year}.csv'
                candidates_path.parent.mkdir(parents=True, exist_ok=True)
                candidates.to_csv(candidates_path, index=False)
                f.write(f"ğŸ“ ì „ì²´ íŠœë‹ í›„ë³´ ëª©ë¡: `{candidates_path}`\n\n")
        
        # 5. ì¼ê´€ì„± ë¶„ì„
        f.write("## ğŸ“ˆ ì‹œë¦¬ì¦ˆ ì¼ê´€ì„± ë¶„ì„\n\n")
        if not trends_df.empty:
            # MAPE í‘œì¤€í¸ì°¨ê°€ ë†’ì€ ì‹œë¦¬ì¦ˆ = ì˜ˆì¸¡ì´ ë¶ˆì•ˆì •
            unstable = metrics_df.groupby('series_id')['mape'].std().reset_index()
            unstable.columns = ['series_id', 'mape_std']
            unstable = unstable.sort_values('mape_std', ascending=False)
            
            f.write("### ì˜ˆì¸¡ ë¶ˆì•ˆì • ì‹œë¦¬ì¦ˆ (MAPE í¸ì°¨ ë†’ìŒ)\n\n")
            f.write("| Series ID | MAPE í‘œì¤€í¸ì°¨ |\n")
            f.write("|-----------|---------------|\n")
            
            for _, row in unstable.head(10).iterrows():
                f.write(f"| {row['series_id']} | {row['mape_std']:.4f} |\n")
            
            f.write("\n")
        
        f.write("---\n\n")
        f.write("## ë‹¤ìŒ ë‹¨ê³„\n\n")
        f.write("1. **ê²½ëŸ‰ ë³´ì • ì ìš©**: Bias Map, Seasonal Recalibration\n")
        f.write("2. **Optuna íŠœë‹**: ìƒìœ„ ìš°ì„ ìˆœìœ„ ì‹œë¦¬ì¦ˆë¶€í„°\n")
        f.write("3. **ì¬í‰ê°€**: ê°œì„  íš¨ê³¼ ì¸¡ì •\n\n")
        f.write("```bash\n")
        f.write("# ë³´ì • íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n")
        f.write(f"python batch.py reconcile --year {end_year + 1} --kpi-mape 0.20\n\n")
        f.write("# Optuna íŠœë‹\n")
        f.write(f"python tools/run_optuna.py --candidates artifacts/metrics/tuning_candidates_rolling_{start_year}_{end_year}.csv\n")
        f.write("```\n")
    
    print(f"\nâœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_file}")


def main():
    parser = argparse.ArgumentParser(description='Rolling ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰')
    parser.add_argument('--start', type=int, required=True, help='ì‹œì‘ ì—°ë„')
    parser.add_argument('--end', type=int, required=True, help='ì¢…ë£Œ ì—°ë„ (í•™ìŠµ)')
    parser.add_argument('--series', type=str, default='all', help='ì‹œë¦¬ì¦ˆ í•„í„°')
    parser.add_argument('--engine', type=str, default='pandas', help='ì‹¤í–‰ ì—”ì§„')
    parser.add_argument('--curated', type=str, default='data/curated/claims.parquet', help='Curated ë°ì´í„° ê²½ë¡œ')
    parser.add_argument('--output', type=str, default='reports', help='ë³´ê³ ì„œ ì¶œë ¥ ë””ë ‰í† ë¦¬')
    
    args = parser.parse_args()
    
    curated_path = Path(args.curated)
    output_path = Path(args.output)
    
    # 1. Rolling ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    metrics_df = run_rolling_backtest(
        curated_path=curated_path,
        start_year=args.start,
        end_year=args.end,
        series_filter=args.series,
        engine=args.engine
    )
    
    if metrics_df.empty:
        print("\nâš ï¸  ë©”íŠ¸ë¦­ì´ ê³„ì‚°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # 2. íŠ¸ë Œë“œ ë¶„ì„
    print(f"\n{'='*70}")
    print("íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
    print(f"{'='*70}\n")
    
    trends_df = analyze_trends(metrics_df)
    
    # 3. ê²°ê³¼ ì €ì¥
    print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    metrics_path = Path('artifacts/metrics')
    metrics_path.mkdir(parents=True, exist_ok=True)
    
    # ë©”íŠ¸ë¦­ ì €ì¥
    metrics_file = metrics_path / f'rolling_metrics_{args.start}_{args.end}.parquet'
    metrics_df.to_parquet(metrics_file, index=False)
    print(f"   âœ“ {metrics_file}")
    
    # íŠ¸ë Œë“œ ì €ì¥
    if not trends_df.empty:
        trends_file = metrics_path / f'rolling_trends_{args.start}_{args.end}.parquet'
        trends_df.to_parquet(trends_file, index=False)
        print(f"   âœ“ {trends_file}")
    
    # 4. ë³´ê³ ì„œ ìƒì„±
    print("\nğŸ“ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    generate_rolling_report(
        metrics_df=metrics_df,
        trends_df=trends_df,
        start_year=args.start,
        end_year=args.end,
        output_path=output_path
    )
    
    # 5. ìš”ì•½ ì¶œë ¥
    print(f"\n{'='*70}")
    print("Rolling ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print(f"{'='*70}\n")
    
    print(f"ì´ í…ŒìŠ¤íŠ¸ ê¸°ê°„: {args.start} ~ {args.end + 1}")
    print(f"ì´ ì‹œë¦¬ì¦ˆ: {metrics_df['series_id'].nunique()}")
    print(f"ì´ ê´€ì¸¡ì¹˜: {len(metrics_df)}")
    
    print(f"\nì „ì²´ í‰ê·  ì„±ëŠ¥:")
    print(f"  MAPE: {metrics_df['mape'].mean():.4f}")
    print(f"  Bias: {metrics_df['bias'].mean():.4f}")
    print(f"  MAE: {metrics_df['mae'].mean():.2f}")
    
    if not trends_df.empty:
        degrading_count = trends_df['is_degrading'].sum()
        print(f"\nì„±ëŠ¥ ì•…í™” ì‹œë¦¬ì¦ˆ: {degrading_count}ê°œ ({degrading_count/len(trends_df)*100:.1f}%)")
    
    print(f"\n{'='*70}\n")


if __name__ == '__main__':
    main()

import pandas as pd, numpy as np
from pathlib import Path
from io_utils import read_parquet, write_parquet, log_jsonl, ART
from forecasting import fit_sarimax, load_model, save_artifacts
from src.reconcile import BiasCorrector, SeasonalRecalibrator, ChangepointDetector
from src.guards import check_sparsity, check_drift, check_completeness
from src.metrics import compute_all_metrics

def metrics_table(y_true, y_pred):
    eps = 1e-9
    mape = (np.abs((y_true - y_pred) / np.maximum(y_true, eps))).mean()
    bias = (y_pred - y_true).mean() / (np.maximum(y_true.mean(), eps))
    rmse = np.sqrt(((y_true - y_pred)**2).mean())
    denom = (np.abs(np.diff(y_true))).mean() or 1.0
    mase = (np.abs(y_true - y_pred).mean()) / denom
    return mape, mase, bias, rmse

def reconcile_year(curated_path, year, kpi_mape=0.20, apply_guards=True, apply_bias=True, apply_seasonal=True):
    """
    í†µí•© ë³´ì • íŒŒì´í”„ë¼ì¸
    
    Args:
        curated_path: Curated ë°ì´í„° ê²½ë¡œ
        year: ë³´ì • ëŒ€ìƒ ì—°ë„
        kpi_mape: MAPE ì„ê³„ê°’
        apply_guards: ê°€ë“œ ì²´í¬ ì ìš© ì—¬ë¶€
        apply_bias: Bias ë³´ì • ì ìš© ì—¬ë¶€
        apply_seasonal: ê³„ì ˆì„± ì¬ë³´ì • ì ìš© ì—¬ë¶€
    """
    df = read_parquet(curated_path)
    fc = read_parquet(ART/f"forecasts/{year}.parquet")
    
    out_metrics = []
    adjustments = {}
    
    print(f"\n{'='*70}")
    print(f"Reconcile íŒŒì´í”„ë¼ì¸ - {year}ë…„")
    print(f"{'='*70}\n")
    
    for series, g_true in df[df["year"]==year].groupby("series_id"):
        g_pred = fc[fc["series_id"]==series].sort_values("week")
        y_true = g_true.sort_values("week")["claim_count"].values
        y_pred = g_pred["y_pred"].values
        
        # ì£¼ì°¨ ì •ë³´ ì¶”ì¶œ
        week_info = g_true.sort_values("week")["week"].values
        
        # ì´ˆê¸° ë©”íŠ¸ë¦­ ê³„ì‚°
        mape, mase, bias, rmse = metrics_table(y_true, y_pred)
        
        print(f"\nğŸ“Š {series}")
        print(f"   ì´ˆê¸° MAPE: {mape:.4f}, Bias: {bias:.4f}")
        
        # ê°€ë“œ ì²´í¬
        guard_flags = {}
        if apply_guards:
            # í•™ìŠµ ë°ì´í„° ì¶”ì¶œ
            hist = df[df["series_id"]==series].sort_values(["year","week"])
            y_hist = hist[hist["year"]<=year-1]["claim_count"].values
            
            # í¬ì†Œë„ ì²´í¬
            is_sparse = check_sparsity(y_hist, threshold=0.8)
            has_drift = check_drift(y_hist, window=52)
            is_complete = check_completeness(y_hist, expected_length=52)
            
            guard_flags = {
                'is_sparse': is_sparse,
                'has_drift': has_drift,
                'is_complete': is_complete
            }
            
            if is_sparse:
                print(f"   âš ï¸  í¬ì†Œ ì‹œë¦¬ì¦ˆ ê°ì§€ - Naive ëª¨ë¸ ê¶Œì¥")
            if has_drift:
                print(f"   âš ï¸  ë“œë¦¬í”„íŠ¸ ê°ì§€ - ì¬ë³´ì • í•„ìš”")
            if not is_complete:
                print(f"   âš ï¸  ë¶ˆì™„ì „í•œ ë°ì´í„° - ë³´ì • ë³´ë¥˜")
        
        # ë³´ì • ì ìš©
        y_adjusted = y_pred.copy()
        adj_metadata = {
            'bias_adj': False,
            'seasonal_recal': False,
            'changepoints': [],
            'guards': guard_flags
        }
        
        # 1. Bias ë³´ì •
        if apply_bias and mape > 0.10 and not guard_flags.get('is_sparse', False):
            try:
                corrector = BiasCorrector(method='weekly')
                y_adjusted = corrector.fit_transform(y_adjusted, y_true, week_info)
                adj_metadata['bias_adj'] = True
                
                # ë³´ì • í›„ ë©”íŠ¸ë¦­
                mape_adj, _, bias_adj, _ = metrics_table(y_true, y_adjusted)
                print(f"   âœ“ Bias ë³´ì • ì ìš© - MAPE: {mape:.4f} â†’ {mape_adj:.4f}")
                
            except Exception as e:
                print(f"   âœ— Bias ë³´ì • ì‹¤íŒ¨: {e}")
        
        # 2. ê³„ì ˆì„± ì¬ë³´ì •
        if apply_seasonal and mape > kpi_mape and not guard_flags.get('is_sparse', False):
            try:
                hist = df[df["series_id"]==series].sort_values(["year","week"])
                y_hist = hist[hist["year"]<=year-1]["claim_count"].values
                
                recalibrator = SeasonalRecalibrator(recent_years=2)
                y_adjusted = recalibrator.fit_transform(y_hist, y_adjusted)
                adj_metadata['seasonal_recal'] = True
                
                # ë³´ì • í›„ ë©”íŠ¸ë¦­
                mape_adj, _, bias_adj, _ = metrics_table(y_true, y_adjusted)
                print(f"   âœ“ ê³„ì ˆì„± ì¬ë³´ì • ì ìš© - MAPE: {mape:.4f} â†’ {mape_adj:.4f}")
                
            except Exception as e:
                print(f"   âœ— ê³„ì ˆì„± ì¬ë³´ì • ì‹¤íŒ¨: {e}")
        
        # 3. ë³€í™”ì  ê°ì§€
        try:
            hist = df[df["series_id"]==series].sort_values(["year","week"])
            y_hist = hist[hist["year"]<=year-1]["claim_count"].values
            
            detector = ChangepointDetector(method='statistical')
            changepoints = detector.detect(y_hist)
            adj_metadata['changepoints'] = changepoints.tolist()
            
            if len(changepoints) > 0:
                print(f"   âš ï¸  {len(changepoints)}ê°œ ë³€í™”ì  ê°ì§€")
        except Exception as e:
            print(f"   âš ï¸  ë³€í™”ì  ê°ì§€ ì‹¤íŒ¨: {e}")
        
        # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
        final_metrics = compute_all_metrics(y_true, y_adjusted, y_hist if 'y_hist' in locals() else None)
        
        out_metrics.append({
            "series_id": series,
            "year": year,
            "MAPE_original": mape,
            "MAPE_adjusted": final_metrics['mape'],
            "MASE": final_metrics.get('mase', np.nan),
            "Bias_original": bias,
            "Bias_adjusted": final_metrics['bias'],
            "RMSE": final_metrics['rmse'],
            "n_points": len(y_true),
            "bias_adj_applied": adj_metadata['bias_adj'],
            "seasonal_recal_applied": adj_metadata['seasonal_recal'],
            "n_changepoints": len(adj_metadata['changepoints'])
        })
        
        # ë³´ì • ì •ë³´ ì €ì¥
        bias_intercept = float((y_true - y_pred).mean())
        adjustments[series] = {
            "bias_intercept": bias_intercept,
            "metadata": adj_metadata
        }
        
        # í•„ìš” ì‹œ ì¬í•™ìŠµ (MAPEê°€ ì—¬ì „íˆ ë†’ì€ ê²½ìš°)
        if final_metrics['mape'] > kpi_mape and not guard_flags.get('is_sparse', False):
            try:
                hist = df[df["series_id"]==series].sort_values(["year","week"])
                y_hist = hist[hist["year"]<=year-1]["claim_count"].reset_index(drop=True)
                model2 = fit_sarimax(y_hist)
                save_artifacts(series, year-1, model2)
                print(f"   ğŸ”„ ì¬í•™ìŠµ ì™„ë£Œ")
            except Exception as e:
                log_jsonl({"event":"reseason_fail","series":series,"year":year,"error":str(e)})
                print(f"   âœ— ì¬í•™ìŠµ ì‹¤íŒ¨: {e}")
    
    # ê²°ê³¼ ì €ì¥
    met = pd.DataFrame(out_metrics)
    write_parquet(met, ART/f"metrics/metrics_{year}.parquet")
    
    import json
    adj_path = ART/f"adjustments/{year}.json"
    adj_path.parent.mkdir(parents=True, exist_ok=True)
    adj_path.write_text(json.dumps(adjustments, ensure_ascii=False, indent=2), encoding="utf-8")
    
    # ìš”ì•½ ì¶œë ¥
    print(f"\n{'='*70}")
    print("Reconcile ì™„ë£Œ")
    print(f"{'='*70}\n")
    
    print(f"ì´ ì‹œë¦¬ì¦ˆ: {met['series_id'].nunique()}")
    print(f"í‰ê·  MAPE ê°œì„ : {met['MAPE_original'].mean():.4f} â†’ {met['MAPE_adjusted'].mean():.4f}")
    print(f"Bias ë³´ì • ì ìš©: {met['bias_adj_applied'].sum()}ê°œ ì‹œë¦¬ì¦ˆ")
    print(f"ê³„ì ˆì„± ì¬ë³´ì • ì ìš©: {met['seasonal_recal_applied'].sum()}ê°œ ì‹œë¦¬ì¦ˆ")
    
    log_jsonl({"event":"reconcile","year":year,"ok":True,"n_series":met['series_id'].nunique()})
    
    return met
